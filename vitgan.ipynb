{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "import models\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import Dataset, Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define both the classes for Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrypkoDataset(Dataset):\n",
    "    def __init__(self, fnames, transform):\n",
    "        self.transform = transform\n",
    "        self.fnames = fnames\n",
    "        self.num_samples = len(self.fnames)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        fname = self.fnames[idx]\n",
    "        # 1. Load the image\n",
    "        img = torchvision.io.read_image(fname)\n",
    "        # 2. Resize and normalize the images using torchvision.\n",
    "        img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "\n",
    "class InfiniteSampler(Sampler):\n",
    "    def __init__(self, data_source):\n",
    "        super(InfiniteSampler, self).__init__(data_source)\n",
    "        self.N = len(data_source)\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            for idx in torch.randperm(self.N):\n",
    "                yield idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SLN(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-modulated LayerNorm\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features):\n",
    "        super(SLN, self).__init__()\n",
    "        self.ln = nn.LayerNorm(num_features)\n",
    "        # self.gamma = nn.Parameter(torch.FloatTensor(1, 1, 1))\n",
    "        # self.beta = nn.Parameter(torch.FloatTensor(1, 1, 1))\n",
    "        self.gamma = nn.Parameter(torch.randn(1, 1, 1)) #.to(\"cuda\")\n",
    "        self.beta = nn.Parameter(torch.randn(1, 1, 1)) #.to(\"cuda\")\n",
    "\n",
    "    def forward(self, hl, w):\n",
    "        return self.gamma * w * self.ln(hl) + self.beta * w\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_feat, hid_feat = None, out_feat = None, dropout = 0.):\n",
    "        super().__init__()\n",
    "        if not hid_feat:\n",
    "            hid_feat = in_feat\n",
    "        if not out_feat:\n",
    "            out_feat = in_feat\n",
    "        self.linear1 = nn.Linear(in_feat, hid_feat)\n",
    "        self.activation = nn.GELU()\n",
    "        self.linear2 = nn.Linear(hid_feat, out_feat)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implement multi head self attention layer using the \"Einstein summation convention\".\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim:\n",
    "        Token's dimension, EX: word embedding vector size\n",
    "    num_heads:\n",
    "        The number of distinct representations to learn\n",
    "    dim_head:\n",
    "        The dimension of the each head\n",
    "    discriminator:\n",
    "        Used in discriminator or not.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads = 4, dim_head = None, discriminator = False):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_head = int(dim / num_heads) if dim_head is None else dim_head\n",
    "        self.weight_dim = self.num_heads * self.dim_head\n",
    "        self.to_qkv = nn.Linear(dim, self.weight_dim * 3, bias = False)\n",
    "        self.scale_factor = dim ** -0.5\n",
    "        self.discriminator = discriminator\n",
    "        self.w_out = nn.Linear(self.weight_dim, dim, bias = True)\n",
    "\n",
    "        if discriminator:\n",
    "            u, s, v = torch.svd(self.to_qkv.weight)\n",
    "            self.init_spect_norm = torch.max(s)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.dim() == 3\n",
    "\n",
    "        if self.discriminator:\n",
    "            u, s, v = torch.svd(self.to_qkv.weight)\n",
    "            self.to_qkv.weight = torch.nn.Parameter(self.to_qkv.weight * self.init_spect_norm / torch.max(s))\n",
    "\n",
    "        # Generate the q, k, v vectors\n",
    "        qkv = self.to_qkv(x)\n",
    "        q, k, v = tuple(rearrange(qkv, 'b t (d k h) -> k b h t d', k = 3, h = self.num_heads))\n",
    "\n",
    "        # Enforcing Lipschitzness of Transformer Discriminator\n",
    "        # Due to Lipschitz constant of standard dot product self-attention\n",
    "        # layer can be unbounded, so adopt the l2 attention replace the dot product.\n",
    "        if self.discriminator:\n",
    "            attn = torch.cdist(q, k, p = 2)\n",
    "        else:\n",
    "            attn = torch.einsum(\"... i d, ... j d -> ... i j\", q, k)\n",
    "        scale_attn = attn * self.scale_factor\n",
    "        scale_attn_score = torch.softmax(scale_attn, dim = -1)\n",
    "        result = torch.einsum(\"... i j, ... j d -> ... i d\", scale_attn_score, v)\n",
    "\n",
    "        # re-compose\n",
    "        result = rearrange(result, \"b h t d -> b t (h d)\")\n",
    "        return self.w_out(result)\n",
    "\n",
    "\n",
    "class DEncoderBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads = 4, dim_head = None,\n",
    "        dropout = 0., mlp_ratio = 4):\n",
    "        super(DEncoderBlock, self).__init__()\n",
    "        self.attn = Attention(dim, num_heads, dim_head, discriminator = True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "        self.mlp = MLP(dim, dim * mlp_ratio, dropout = dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.norm1(x)\n",
    "        x = x + self.dropout(self.attn(x1))\n",
    "        x2 = self.norm2(x)\n",
    "        x = x + self.mlp(x2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GEncoderBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads = 4, dim_head = None,\n",
    "        dropout = 0., mlp_ratio = 4):\n",
    "        super(GEncoderBlock, self).__init__()\n",
    "        self.attn = Attention(dim, num_heads, dim_head)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm1 = SLN(dim)\n",
    "        self.norm2 = SLN(dim)\n",
    "\n",
    "        self.mlp = MLP(dim, dim * mlp_ratio, dropout = dropout)\n",
    "\n",
    "    def forward(self, hl, x):\n",
    "        hl_temp = self.dropout(self.attn(self.norm1(hl, x))) + hl\n",
    "        hl_final = self.mlp(self.norm2(hl_temp, x)) + hl_temp\n",
    "        return x, hl_final\n",
    "\n",
    "\n",
    "class GTransformerEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "        dim,\n",
    "        blocks = 6,\n",
    "        num_heads = 8,\n",
    "        dim_head = None,\n",
    "        dropout = 0\n",
    "    ):\n",
    "        super(GTransformerEncoder, self).__init__()\n",
    "        self.blocks = self._make_layers(dim, blocks, num_heads, dim_head, dropout)\n",
    "\n",
    "    def _make_layers(self,\n",
    "        dim,\n",
    "        blocks = 6,\n",
    "        num_heads = 8,\n",
    "        dim_head = None,\n",
    "        dropout = 0\n",
    "    ):\n",
    "        layers = []\n",
    "        for _ in range(blocks):\n",
    "            layers.append(GEncoderBlock(dim, num_heads, dim_head, dropout))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, hl, x):\n",
    "        for block in self.blocks:\n",
    "            x, hl = block(hl, x)\n",
    "        return x, hl\n",
    "\n",
    "\n",
    "class DTransformerEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "        dim,\n",
    "        blocks = 6,\n",
    "        num_heads = 8,\n",
    "        dim_head = None,\n",
    "        dropout = 0\n",
    "    ):\n",
    "        super(DTransformerEncoder, self).__init__()\n",
    "        self.blocks = self._make_layers(dim, blocks, num_heads, dim_head, dropout)\n",
    "\n",
    "    def _make_layers(self,\n",
    "        dim,\n",
    "        blocks = 6,\n",
    "        num_heads = 8,\n",
    "        dim_head = None,\n",
    "        dropout = 0\n",
    "    ):\n",
    "        layers = []\n",
    "        for _ in range(blocks):\n",
    "            layers.append(DEncoderBlock(dim, num_heads, dim_head, dropout))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SineLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper: Implicit Neural Representation with Periodic Activ ation Function (SIREN)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, bias = True,is_first = False, omega_0 = 30):\n",
    "        super().__init__()\n",
    "        self.omega_0 = omega_0\n",
    "        self.is_first = is_first\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        with torch.no_grad():\n",
    "            if self.is_first:\n",
    "                self.linear.weight.uniform_(-1 / self.in_features, 1 / self.in_features)\n",
    "            else:\n",
    "                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0, np.sqrt(6 / self.in_features) / self.omega_0)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return torch.sin(self.omega_0 * self.linear(input))\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self,\n",
    "        initialize_size = 8,\n",
    "        dim = 384,\n",
    "        blocks = 6,\n",
    "        num_heads = 6,\n",
    "        dim_head = None,\n",
    "        dropout = 0,\n",
    "        out_channels = 3\n",
    "    ):\n",
    "        super(Generator, self).__init__()\n",
    "        self.initialize_size = initialize_size\n",
    "        self.dim = dim\n",
    "        self.blocks = blocks\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_head = dim_head\n",
    "        self.dropout = dropout\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.pos_emb1D = nn.Parameter(torch.randn(self.initialize_size * 8, dim))\n",
    "\n",
    "        self.mlp = nn.Linear(1024, (self.initialize_size * 8) * self.dim)\n",
    "        self.Transformer_Encoder = GTransformerEncoder(dim, blocks, num_heads, dim_head, dropout)\n",
    "\n",
    "        # Implicit Neural Representation\n",
    "        self.w_out = nn.Sequential(\n",
    "            SineLayer(dim, dim * 2, is_first = True, omega_0 = 30.),\n",
    "            SineLayer(dim * 2, self.initialize_size * 8 * self.out_channels, is_first = False, omega_0 = 30)\n",
    "        )\n",
    "        self.sln_norm = SLN(self.dim)\n",
    "\n",
    "    def forward(self, noise):\n",
    "        x = self.mlp(noise).view(-1, self.initialize_size * 8, self.dim)\n",
    "        x, hl = self.Transformer_Encoder(self.pos_emb1D, x)\n",
    "        x = self.sln_norm(hl, x)\n",
    "        x = self.w_out(x)  # Replace to siren\n",
    "        result = x.view(x.shape[0], 3, self.initialize_size * 8, self.initialize_size * 8)\n",
    "        return result\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,\n",
    "        in_channels = 3,\n",
    "        patch_size = 8,\n",
    "        extend_size = 2,\n",
    "        dim = 384,\n",
    "        blocks = 6,\n",
    "        num_heads = 6,\n",
    "        dim_head = None,\n",
    "        dropout = 0\n",
    "    ):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.patch_size = patch_size + 2 * extend_size\n",
    "        self.token_dim = in_channels * (self.patch_size ** 2)\n",
    "        self.project_patches = nn.Linear(self.token_dim, dim)\n",
    "\n",
    "        self.emb_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.pos_emb1D = nn.Parameter(torch.randn(self.token_dim + 1, dim))\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, 1)\n",
    "        )\n",
    "\n",
    "        self.Transformer_Encoder = DTransformerEncoder(dim, blocks, num_heads, dim_head, dropout)\n",
    "\n",
    "\n",
    "    def forward(self, img):\n",
    "        # Generate overlappimg image patches\n",
    "        stride_h = (img.shape[2] - self.patch_size) // 8 + 1\n",
    "        stride_w = (img.shape[3] - self.patch_size) // 8 + 1\n",
    "        img_patches = img.unfold(2, self.patch_size, stride_h).unfold(3, self.patch_size, stride_w)\n",
    "        img_patches = img_patches.contiguous().view(\n",
    "            img_patches.shape[0], img_patches.shape[2] * img_patches.shape[3], img_patches.shape[1] * img_patches.shape[4] * img_patches.shape[5]\n",
    "        )\n",
    "        img_patches = self.project_patches(img_patches)\n",
    "        batch_size, tokens, _ = img_patches.shape\n",
    "\n",
    "        # Prepend the classifier token\n",
    "        cls_token = repeat(self.cls_token, '() n d -> b n d', b = batch_size)\n",
    "        img_patches = torch.cat((cls_token, img_patches), dim = 1)\n",
    "\n",
    "        # Plus the positional embedding\n",
    "        img_patches = img_patches + self.pos_emb1D[: tokens + 1, :]\n",
    "        img_patches = self.emb_dropout(img_patches)\n",
    "\n",
    "        result = self.Transformer_Encoder(img_patches)\n",
    "        logits = self.mlp_head(result[:, 0, :])\n",
    "        logits = nn.Sigmoid()(logits)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def test_both():\n",
    "    B, dim = 10, 1024\n",
    "    G = Generator(initialize_size = 8, dropout = 0.1)\n",
    "    noise = torch.FloatTensor(np.random.normal(0, 1, (B, dim)))\n",
    "    fake_img = G(noise)\n",
    "    D = Discriminator(patch_size = 8, dropout = 0.1)\n",
    "    D_logits = D(fake_img)\n",
    "    print(D_logits)\n",
    "    print(f\"Max: {torch.max(D_logits)}, Min: {torch.min(D_logits)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def exp_mov_avg(Gs, G, alpha = 0.999, global_step = 999):\n",
    "    alpha = min(1 - 1 / (global_step + 1), alpha)\n",
    "    for ema_param, param in zip(Gs.parameters(), G.parameters()):\n",
    "        ema_param.data.mul_(alpha).add_(1 - alpha, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator, generator_s, discriminator, optim_g, optim_d, data_loader, device):\n",
    "    fixed_noise = torch.FloatTensor(np.random.normal(0, 1, (16, args.latent_dim))).to(device)\n",
    "    for step in tqdm(range(args.steps + 1)):\n",
    "        # Train Discriminator\n",
    "        optim_d.zero_grad()\n",
    "\n",
    "        # Forward + Backward with real images\n",
    "        r_img = next(data_loader).to(device)\n",
    "        r_label = torch.ones(args.batch_size).to(device)\n",
    "        r_logit = discriminator(r_img).flatten()\n",
    "        lossD_real = criterion(r_logit, r_label)\n",
    "        lossD_real.backward()\n",
    "\n",
    "        # Forward + Backward with fake images\n",
    "        latent_vector = torch.FloatTensor(np.random.normal(0, 1, (args.batch_size, args.latent_dim))).to(device)\n",
    "        f_img = generator(latent_vector)\n",
    "        f_label = torch.zeros(args.batch_size).to(device)\n",
    "        f_logit = discriminator(f_img).flatten()\n",
    "        lossD_fake = criterion(f_logit, f_label)\n",
    "        lossD_fake.backward()\n",
    "\n",
    "        optim_d.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optim_g.zero_grad()\n",
    "        f_img = generator(torch.FloatTensor(np.random.normal(0, 1, (args.batch_size, args.latent_dim))).to(device))\n",
    "        r_label = torch.ones(args.batch_size).to(device)\n",
    "        f_logit = discriminator(f_img).flatten()\n",
    "        lossG = criterion(f_logit, r_label)\n",
    "        lossG.backward()\n",
    "        optim_g.step()\n",
    "\n",
    "        exp_mov_avg(generator_s, generator, global_step = step)\n",
    "\n",
    "        if step % args.sample_interval == 0:\n",
    "            generator.eval()\n",
    "            vis = generator(fixed_noise).detach().cpu()\n",
    "            vis = make_grid(vis, nrow = 4, padding = 5, normalize = True)\n",
    "            vis = T.ToPILImage()(vis)\n",
    "            vis.save('samples/vis{:05d}.jpg'.format(step))\n",
    "            generator.train()\n",
    "            print(\"Save sample to samples/vis{:05d}.jpg\".format(step))\n",
    "\n",
    "        if (step + 1) % args.sample_interval == 0 or step == 0:\n",
    "            # Save the checkpoints.\n",
    "            torch.save(generator.state_dict(), 'weights/Generator.pth')\n",
    "            torch.save(generator_s.state_dict(), 'weights/Generator_ema.pth')\n",
    "            torch.save(discriminator.state_dict(), 'weights/Discriminator.pth')\n",
    "            print(\"Save model state.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'lr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kaasa\\Documents\\GitHub\\ViTGAN\\vitgan.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kaasa/Documents/GitHub/ViTGAN/vitgan.ipynb#W6sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mBCELoss()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kaasa/Documents/GitHub/ViTGAN/vitgan.ipynb#W6sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39m# Optimizer and lr_scheduler\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kaasa/Documents/GitHub/ViTGAN/vitgan.ipynb#W6sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m optimizer_g \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(netG\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39;49mlr,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kaasa/Documents/GitHub/ViTGAN/vitgan.ipynb#W6sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m         betas \u001b[39m=\u001b[39m (args\u001b[39m.\u001b[39mbeta1, args\u001b[39m.\u001b[39mbeta2)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kaasa/Documents/GitHub/ViTGAN/vitgan.ipynb#W6sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kaasa/Documents/GitHub/ViTGAN/vitgan.ipynb#W6sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m optimizer_d \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(netD\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mlr,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kaasa/Documents/GitHub/ViTGAN/vitgan.ipynb#W6sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m         betas \u001b[39m=\u001b[39m (args\u001b[39m.\u001b[39mbeta1, args\u001b[39m.\u001b[39mbeta2)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kaasa/Documents/GitHub/ViTGAN/vitgan.ipynb#W6sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kaasa/Documents/GitHub/ViTGAN/vitgan.ipynb#W6sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     \u001b[39m# Start Training\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'lr'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--steps STEPS] [--batch-size BATCH_SIZE]\n",
      "                             [--lr LR] [--beta1 BETA1] [--beta2 BETA2]\n",
      "                             [--latent-dim LATENT_DIM] [--data-dir DATA_DIR]\n",
      "                             [--sample-interval SAMPLE_INTERVAL]\n",
      "                             [--gpu-id GPU_ID]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9003 --control=9001 --hb=9000 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"9e4c7348-b2b6-42bd-be55-e35489029aa7\" --shell=9002 --transport=\"tcp\" --iopub=9004 --f=c:\\Users\\kaasa\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-50322M0JJyjWaZIi.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "%tb\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--steps\", type = int, default = 100000,\n",
    "                        help = \"Number of steps for training (Default: 100000)\")\n",
    "parser.add_argument(\"--batch-size\", type = int, default = 128,\n",
    "                        help = \"Size of each batches (Default: 128)\")\n",
    "parser.add_argument(\"--lr\", type = float, default = 0.002,\n",
    "                        help = \"Learning rate (Default: 0.002)\")\n",
    "parser.add_argument(\"--beta1\", type = float, default = 0.0,\n",
    "                        help = \"Coefficients used for computing running averages of gradient and its square\")\n",
    "parser.add_argument(\"--beta2\", type = float, default = 0.99,\n",
    "                        help = \"Coefficients used for computing running averages of gradient and its square\")\n",
    "parser.add_argument(\"--latent-dim\", type = int, default = 1024,\n",
    "                        help = \"Dimension of the latent vector\")\n",
    "parser.add_argument(\"--data-dir\", type = str, default = \"crypko_data/faces/\",\n",
    "                        help = \"Data root dir of your training data\")\n",
    "parser.add_argument(\"--sample-interval\", type = int, default = 1000,\n",
    "                        help = \"Interval for sampling image from generator\")\n",
    "parser.add_argument(\"--gpu-id\", type = int, default = 1,\n",
    "                        help = \"Select the specific gpu to training\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "    # Device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_id)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Dataloader\n",
    "data_loader = utils.get_dataloader(args.data_dir, batch_size = args.batch_size)\n",
    "\n",
    "\n",
    "    # Create the log folder\n",
    "os.makedirs(\"weights\", exist_ok = True)\n",
    "os.makedirs(\"samples\", exist_ok = True)\n",
    "\n",
    "    # Initialize Generator and Discriminator\n",
    "netG = models.Generator().to(device)\n",
    "netG_s = copy.deepcopy(netG)\n",
    "netD = models.Discriminator().to(device)\n",
    "\n",
    "    # Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "    # Optimizer and lr_scheduler\n",
    "optimizer_g = torch.optim.Adam(netG.parameters(), lr = args.lr,\n",
    "        betas = (args.beta1, args.beta2)\n",
    "    )\n",
    "optimizer_d = torch.optim.Adam(netD.parameters(), lr = args.lr,\n",
    "        betas = (args.beta1, args.beta2)\n",
    "    )\n",
    "\n",
    "    # Start Training\n",
    "train(netG, netG_s, netD, optimizer_g, optimizer_d, data_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
